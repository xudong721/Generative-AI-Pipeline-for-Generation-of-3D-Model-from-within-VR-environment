{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6c4c41d-c6a5-47ab-85ec-71598c0c1c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa120h/miniforge3/envs/eval3d-clean/lib/python3.11/site-packages/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3D Model Evaluation - GTX 1080 Ti Optimized\n",
      "============================================================\n",
      "\n",
      "✓ CUDA available: NVIDIA GeForce GTX 1080 Ti\n",
      "✓ CUDA version: 11.8\n",
      "✓ Available memory: 11.71 GB\n",
      "\n",
      "Loading models...\n",
      "  Model 1: 3d_eval_project/models/tripo_ai/hair dry.obj\n",
      "  Model 2: 3d_eval_project/models/hunyuan3D/hair hunyuan.obj\n",
      "✓ Models loaded successfully\n",
      "  Model 1: 414278 vertices, 783386 faces\n",
      "  Model 2: 840587 vertices, 1499542 faces\n",
      "\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "1. Geometry: Chamfer Distance\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "✓ Chamfer Distance = 0.502942\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "2. Topology: Mesh Quality Metrics\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "  Warning during topology computation: boolean index did not match indexed array along dimension 0; dimension is 1197363 but corresponding boolean dimension is 2350158\n",
      "  Warning during topology computation: boolean index did not match indexed array along dimension 0; dimension is 2337444 but corresponding boolean dimension is 4498626\n",
      "\n",
      "  Model 1 Topology Metrics:\n",
      "  ──────────────────────────────────────────────────\n",
      "  Watertight:              False\n",
      "  Euler Characteristic:    301\n",
      "\n",
      "  Quality Indicators (0.0 is perfect):\n",
      "    ❌ Non-manifold edges....... 0.654254\n",
      "    ✓ Degenerate faces......... 0.000008\n",
      "    ✓ Inconsistent normals..... 0.000000\n",
      "    ✓ Duplicate faces.......... 0.000000\n",
      "    ✓ Isolated vertices........ 0.000000\n",
      "  ──────────────────────────────────────────────────\n",
      "\n",
      "  Model 2 Topology Metrics:\n",
      "  ──────────────────────────────────────────────────\n",
      "  Watertight:              False\n",
      "  Euler Characteristic:    2685\n",
      "\n",
      "  Quality Indicators (0.0 is perfect):\n",
      "    ❌ Non-manifold edges....... 0.641531\n",
      "    ✓ Degenerate faces......... 0.000001\n",
      "    ✓ Inconsistent normals..... 0.000000\n",
      "    ✓ Duplicate faces.......... 0.000000\n",
      "    ✓ Isolated vertices........ 0.000000\n",
      "  ──────────────────────────────────────────────────\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "3. Perceptual: CLIP Similarity\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "Using GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "Loading CLIP model...\n",
      "Rendering views...\n",
      "✓ CLIP Similarity = 0.8785\n",
      "\n",
      "============================================================\n",
      "Summary\n",
      "============================================================\n",
      "  Chamfer Distance:     0.502942\n",
      "\n",
      "  Topology Quality (lower is better):\n",
      "    Non Manifold Edge Ratio  : M1=0.6543, M2=0.6415\n",
      "    Degenerate Face Ratio    : M1=0.0000, M2=0.0000\n",
      "    Duplicate Face Ratio     : M1=0.0000, M2=0.0000\n",
      "\n",
      "  CLIP Similarity:      0.8785\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "eval_no_viewer_gtx1080ti.py\n",
    "\n",
    "Optimized for GTX 1080 Ti (11GB VRAM, CUDA 6.1):\n",
    "- Reduced memory footprint for CLIP\n",
    "- FP32 precision (GTX 1080 Ti doesn't efficiently support FP16/BF16)\n",
    "- Batch processing with gradient disabled\n",
    "- CPU fallback options\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import trimesh\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import clip\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.argv = [x for x in sys.argv if x != \"-f\"]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Geometry: Chamfer Distance\n",
    "# -------------------------\n",
    "def chamfer_distance(mesh1, mesh2, n_samples=5000):\n",
    "    \"\"\"Compute Chamfer Distance with chunked processing for large point clouds\"\"\"\n",
    "    try:\n",
    "        pcd1 = mesh1.sample(n_samples)\n",
    "        pcd2 = mesh2.sample(n_samples)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Sampling failed ({e}), using vertices directly\")\n",
    "        pcd1 = mesh1.vertices[:n_samples]\n",
    "        pcd2 = mesh2.vertices[:n_samples]\n",
    "\n",
    "    # Use chunked processing for very large point clouds\n",
    "    chunk_size = 10000\n",
    "    if len(pcd1) > chunk_size or len(pcd2) > chunk_size:\n",
    "        print(f\"Using chunked Chamfer Distance computation...\")\n",
    "        return chamfer_distance_chunked(pcd1, pcd2, chunk_size)\n",
    "    \n",
    "    tree1 = cKDTree(pcd1)\n",
    "    tree2 = cKDTree(pcd2)\n",
    "\n",
    "    d12, _ = tree1.query(pcd2, k=1)\n",
    "    d21, _ = tree2.query(pcd1, k=1)\n",
    "\n",
    "    cd = float(np.mean(d12) + np.mean(d21))\n",
    "    return cd\n",
    "\n",
    "\n",
    "def chamfer_distance_chunked(pcd1, pcd2, chunk_size=10000):\n",
    "    \"\"\"Chunked version for memory efficiency\"\"\"\n",
    "    tree1 = cKDTree(pcd1)\n",
    "    tree2 = cKDTree(pcd2)\n",
    "    \n",
    "    # Query in chunks\n",
    "    d12_list = []\n",
    "    for i in range(0, len(pcd2), chunk_size):\n",
    "        chunk = pcd2[i:i+chunk_size]\n",
    "        d, _ = tree1.query(chunk, k=1)\n",
    "        d12_list.append(d)\n",
    "    d12 = np.concatenate(d12_list)\n",
    "    \n",
    "    d21_list = []\n",
    "    for i in range(0, len(pcd1), chunk_size):\n",
    "        chunk = pcd1[i:i+chunk_size]\n",
    "        d, _ = tree2.query(chunk, k=1)\n",
    "        d21_list.append(d)\n",
    "    d21 = np.concatenate(d21_list)\n",
    "    \n",
    "    cd = float(np.mean(d12) + np.mean(d21))\n",
    "    return cd\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Topology: Comprehensive mesh quality metrics\n",
    "# -------------------------\n",
    "def topology_metrics(mesh):\n",
    "    \"\"\"\n",
    "    Compute comprehensive topology and quality metrics\n",
    "    Returns a dictionary with multiple quality indicators\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    try:\n",
    "        # 1. Watertightness (is mesh closed?)\n",
    "        metrics['is_watertight'] = mesh.is_watertight\n",
    "        \n",
    "        # 2. Euler characteristic (topological invariant)\n",
    "        # For a closed surface: χ = V - E + F = 2(1 - g), where g is genus\n",
    "        V = len(mesh.vertices)\n",
    "        E = len(mesh.edges_unique)\n",
    "        F = len(mesh.faces)\n",
    "        euler = V - E + F\n",
    "        metrics['euler_characteristic'] = euler\n",
    "        \n",
    "        # Expected genus (0 for sphere, 1 for torus, etc.)\n",
    "        if mesh.is_watertight:\n",
    "            genus = 1 - euler / 2\n",
    "            metrics['genus'] = int(genus) if genus >= 0 else None\n",
    "        else:\n",
    "            metrics['genus'] = None\n",
    "        \n",
    "        # 3. Non-manifold edges and vertices\n",
    "        # Non-manifold edges: edges shared by more than 2 faces or only 1 face\n",
    "        edge_faces = mesh.faces_sparse.tocsr()\n",
    "        edges_per_face_count = np.array((edge_faces > 0).sum(axis=0)).flatten()\n",
    "        \n",
    "        non_manifold_edges = np.sum((edges_per_face_count != 2) & (edges_per_face_count > 0))\n",
    "        metrics['non_manifold_edge_ratio'] = float(non_manifold_edges) / max(1, E)\n",
    "        \n",
    "        # 4. Degenerate faces (zero area)\n",
    "        face_areas = mesh.area_faces\n",
    "        degenerate_faces = np.sum(face_areas < 1e-10)\n",
    "        metrics['degenerate_face_ratio'] = float(degenerate_faces) / max(1, F)\n",
    "        \n",
    "        # 5. Self-intersections check (expensive, optional)\n",
    "        # For large meshes, we skip this or sample\n",
    "        if F < 100000:\n",
    "            is_intersecting = mesh.is_self_intersecting\n",
    "            metrics['has_self_intersection'] = bool(is_intersecting) if is_intersecting is not None else None\n",
    "        else:\n",
    "            metrics['has_self_intersection'] = None  # Too expensive\n",
    "        \n",
    "        # 6. Face orientation consistency\n",
    "        # Check if all faces have consistent normals\n",
    "        try:\n",
    "            mesh_copy = mesh.copy()\n",
    "            mesh_copy.fix_normals()  # This will reorient faces\n",
    "            # Count how many faces needed to be flipped\n",
    "            flipped = np.sum(mesh_copy.face_normals != mesh.face_normals)\n",
    "            metrics['inconsistent_face_ratio'] = float(flipped) / max(1, F)\n",
    "        except:\n",
    "            metrics['inconsistent_face_ratio'] = None\n",
    "        \n",
    "        # 7. Duplicate faces\n",
    "        unique_faces = len(np.unique(np.sort(mesh.faces, axis=1), axis=0))\n",
    "        duplicate_faces = F - unique_faces\n",
    "        metrics['duplicate_face_ratio'] = float(duplicate_faces) / max(1, F)\n",
    "        \n",
    "        # 8. Isolated vertices (vertices not connected to any face)\n",
    "        used_vertices = np.unique(mesh.faces.flatten())\n",
    "        isolated_vertices = V - len(used_vertices)\n",
    "        metrics['isolated_vertex_ratio'] = float(isolated_vertices) / max(1, V)\n",
    "        \n",
    "        # 9. Boundary edges (edges with only one adjacent face)\n",
    "        boundary_edges = mesh.edges_unique[mesh.edges_face.flatten() == -1]\n",
    "        metrics['boundary_edge_ratio'] = float(len(boundary_edges)) / max(1, E)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Warning during topology computation: {e}\")\n",
    "        return metrics\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def print_topology_metrics(metrics, model_name=\"Model\"):\n",
    "    \"\"\"Pretty print topology metrics\"\"\"\n",
    "    print(f\"\\n  {model_name} Topology Metrics:\")\n",
    "    print(f\"  {'─'*50}\")\n",
    "    \n",
    "    # Basic properties\n",
    "    print(f\"  Watertight:              {metrics.get('is_watertight', 'N/A')}\")\n",
    "    print(f\"  Euler Characteristic:    {metrics.get('euler_characteristic', 'N/A')}\")\n",
    "    if metrics.get('genus') is not None:\n",
    "        print(f\"  Genus:                   {metrics.get('genus')} (0=sphere, 1=torus, etc.)\")\n",
    "    \n",
    "    # Quality ratios\n",
    "    print(f\"\\n  Quality Indicators (0.0 is perfect):\")\n",
    "    ratio_metrics = [\n",
    "        ('non_manifold_edge_ratio', 'Non-manifold edges'),\n",
    "        ('degenerate_face_ratio', 'Degenerate faces'),\n",
    "        ('inconsistent_face_ratio', 'Inconsistent normals'),\n",
    "        ('duplicate_face_ratio', 'Duplicate faces'),\n",
    "        ('isolated_vertex_ratio', 'Isolated vertices'),\n",
    "        ('boundary_edge_ratio', 'Boundary edges'),\n",
    "    ]\n",
    "    \n",
    "    for key, label in ratio_metrics:\n",
    "        value = metrics.get(key)\n",
    "        if value is not None:\n",
    "            status = \"✓\" if value < 0.01 else \"⚠️\" if value < 0.1 else \"❌\"\n",
    "            print(f\"    {status} {label:.<25s} {value:.6f}\")\n",
    "    \n",
    "    # Self-intersection\n",
    "    if metrics.get('has_self_intersection') is not None:\n",
    "        status = \"❌\" if metrics['has_self_intersection'] else \"✓\"\n",
    "        print(f\"    {status} Self-intersection: {metrics['has_self_intersection']}\")\n",
    "    \n",
    "    print(f\"  {'─'*50}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CPU renderer for CLIP\n",
    "# -------------------------\n",
    "def render_pointcloud_views(mesh, image_size=224, point_radius=2, views=3):\n",
    "    \"\"\"\n",
    "    CPU-based orthographic projection rendering\n",
    "    Optimized for memory efficiency\n",
    "    \"\"\"\n",
    "    try:\n",
    "        verts = mesh.vertices.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not access vertices ({e})\")\n",
    "        return [Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))] * views\n",
    "    \n",
    "    if len(verts) == 0:\n",
    "        return [Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))] * views\n",
    "    \n",
    "    # Center and normalize\n",
    "    center = verts.mean(axis=0)\n",
    "    verts -= center\n",
    "    \n",
    "    scale = np.max(np.linalg.norm(verts, axis=1))\n",
    "    if scale <= 0:\n",
    "        scale = 1.0\n",
    "    verts = verts / scale\n",
    "    \n",
    "    imgs = []\n",
    "    projections = {\n",
    "        \"front\": verts[:, [0, 1]],\n",
    "        \"side\": verts[:, [2, 1]],\n",
    "        \"top\": verts[:, [0, 2]],\n",
    "    }\n",
    "    \n",
    "    selected = [\"front\", \"side\", \"top\"][:views]\n",
    "    for k in selected:\n",
    "        pts = projections[k]\n",
    "        margin = int(0.05 * image_size)\n",
    "        coords = ((pts + 1.0) * 0.5 * (image_size - 2*margin)) + margin\n",
    "        coords = np.round(coords).astype(int)\n",
    "        coords[:, 0] = np.clip(coords[:, 0], 0, image_size-1)\n",
    "        coords[:, 1] = np.clip(coords[:, 1], 0, image_size-1)\n",
    "        \n",
    "        img = Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw points with anti-aliasing effect\n",
    "        for (x, y) in coords:\n",
    "            x0 = x - point_radius\n",
    "            y0 = image_size - 1 - y - point_radius\n",
    "            x1 = x + point_radius\n",
    "            y1 = image_size - 1 - y + point_radius\n",
    "            draw.ellipse([x0, y0, x1, y1], fill=(30, 30, 30))\n",
    "        \n",
    "        imgs.append(img)\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLIP similarity - GTX 1080 Ti optimized\n",
    "# -------------------------\n",
    "def clip_similarity_from_meshes(mesh1, mesh2, image_size=224, device=None, use_cpu=False):\n",
    "    \"\"\"\n",
    "    CLIP similarity optimized for GTX 1080 Ti\n",
    "    - Reduced image size option\n",
    "    - CPU fallback\n",
    "    - Explicit memory management\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if use_cpu or not torch.cuda.is_available():\n",
    "            device = \"cpu\"\n",
    "            print(\"Using CPU for CLIP (slower but safer for memory)\")\n",
    "        else:\n",
    "            device = \"cuda\"\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    try:\n",
    "        # Clear GPU cache before loading\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Load CLIP model with FP32 (GTX 1080 Ti works best with FP32)\n",
    "        print(\"Loading CLIP model...\")\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "        model.eval()  # Ensure eval mode\n",
    "        \n",
    "        # Render views\n",
    "        print(\"Rendering views...\")\n",
    "        imgs1 = render_pointcloud_views(mesh1, image_size=image_size)\n",
    "        imgs2 = render_pointcloud_views(mesh2, image_size=image_size)\n",
    "        \n",
    "        def combine(imgs):\n",
    "            widths, heights = zip(*(i.size for i in imgs))\n",
    "            total_w = sum(widths)\n",
    "            max_h = max(heights)\n",
    "            out = Image.new(\"RGB\", (total_w, max_h), (255, 255, 255))\n",
    "            x_offset = 0\n",
    "            for im in imgs:\n",
    "                out.paste(im, (x_offset, 0))\n",
    "                x_offset += im.size[0]\n",
    "            return out\n",
    "        \n",
    "        comb1 = combine(imgs1)\n",
    "        comb2 = combine(imgs2)\n",
    "        \n",
    "        # Preprocess images\n",
    "        image1 = preprocess(comb1).unsqueeze(0).to(device)\n",
    "        image2 = preprocess(comb2).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Compute similarity with no gradient\n",
    "        with torch.no_grad():\n",
    "            e1 = model.encode_image(image1).float()  # Explicit FP32\n",
    "            e2 = model.encode_image(image2).float()\n",
    "            \n",
    "            # Normalize\n",
    "            e1 = e1 / e1.norm(dim=-1, keepdim=True)\n",
    "            e2 = e2 / e2.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute similarity\n",
    "            sim = float((e1 @ e2.T).cpu().item())\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, image1, image2, e1, e2\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return sim\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"\\n⚠️ GPU Out of Memory! Retrying with CPU...\")\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            return clip_similarity_from_meshes(mesh1, mesh2, image_size, \"cpu\", True)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main evaluation\n",
    "# -------------------------\n",
    "def evaluate_models(obj1_path, obj2_path, n_samples=3000, use_cpu_clip=False):\n",
    "    \"\"\"\n",
    "    Evaluate two OBJ models\n",
    "    \n",
    "    Args:\n",
    "        obj1_path: Path to first OBJ file\n",
    "        obj2_path: Path to second OBJ file\n",
    "        n_samples: Number of points for Chamfer Distance (reduce if OOM)\n",
    "        use_cpu_clip: Force CPU for CLIP computation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"3D Model Evaluation - GTX 1080 Ti Optimized\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"✓ Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available, using CPU\\n\")\n",
    "    \n",
    "    # Load meshes\n",
    "    print(f\"Loading models...\")\n",
    "    print(f\"  Model 1: {obj1_path}\")\n",
    "    print(f\"  Model 2: {obj2_path}\")\n",
    "    \n",
    "    try:\n",
    "        mesh1 = trimesh.load(obj1_path, force='mesh')\n",
    "        mesh2 = trimesh.load(obj2_path, force='mesh')\n",
    "        print(f\"✓ Models loaded successfully\")\n",
    "        print(f\"  Model 1: {len(mesh1.vertices)} vertices, {len(mesh1.faces)} faces\")\n",
    "        print(f\"  Model 2: {len(mesh2.vertices)} vertices, {len(mesh2.faces)} faces\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading models: {e}\")\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Chamfer Distance\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"1. Geometry: Chamfer Distance\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        cd = chamfer_distance(mesh1, mesh2, n_samples=n_samples)\n",
    "        print(f\"✓ Chamfer Distance = {cd:.6f}\")\n",
    "        results['chamfer'] = cd\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Chamfer Distance failed: {e}\")\n",
    "        results['chamfer'] = None\n",
    "    \n",
    "    # 2. Topology metrics\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"2. Topology: Mesh Quality Metrics\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        topo1 = topology_metrics(mesh1)\n",
    "        topo2 = topology_metrics(mesh2)\n",
    "        \n",
    "        print_topology_metrics(topo1, \"Model 1\")\n",
    "        print_topology_metrics(topo2, \"Model 2\")\n",
    "        \n",
    "        results['topology_1'] = topo1\n",
    "        results['topology_2'] = topo2\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Topology computation failed: {e}\")\n",
    "        results['topology_1'] = None\n",
    "        results['topology_2'] = None\n",
    "    \n",
    "    # 3. CLIP similarity\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"3. Perceptual: CLIP Similarity\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        clip_score = clip_similarity_from_meshes(\n",
    "            mesh1, mesh2, \n",
    "            image_size=224,\n",
    "            use_cpu=use_cpu_clip\n",
    "        )\n",
    "        print(f\"✓ CLIP Similarity = {clip_score:.4f}\")\n",
    "        results['clip'] = clip_score\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CLIP computation failed: {e}\")\n",
    "        results['clip'] = None\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Chamfer Distance\n",
    "    if results.get('chamfer') is not None:\n",
    "        print(f\"  Chamfer Distance:     {results['chamfer']:.6f}\")\n",
    "    \n",
    "    # Topology summary\n",
    "    if results.get('topology_1') and results.get('topology_2'):\n",
    "        print(f\"\\n  Topology Quality (lower is better):\")\n",
    "        t1 = results['topology_1']\n",
    "        t2 = results['topology_2']\n",
    "        \n",
    "        keys = ['non_manifold_edge_ratio', 'degenerate_face_ratio', \n",
    "                'duplicate_face_ratio', 'boundary_edge_ratio']\n",
    "        for key in keys:\n",
    "            v1 = t1.get(key, 'N/A')\n",
    "            v2 = t2.get(key, 'N/A')\n",
    "            if isinstance(v1, (int, float)) and isinstance(v2, (int, float)):\n",
    "                label = key.replace('_', ' ').title()\n",
    "                print(f\"    {label:25s}: M1={v1:.4f}, M2={v2:.4f}\")\n",
    "    \n",
    "    # CLIP score\n",
    "    if results.get('clip') is not None:\n",
    "        print(f\"\\n  CLIP Similarity:      {results['clip']:.4f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Entry point\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Parse arguments\n",
    "    if len(sys.argv) >= 3:\n",
    "        obj1 = sys.argv[1]\n",
    "        obj2 = sys.argv[2]\n",
    "    else:\n",
    "        obj1 = \"3d_eval_project/models/tripo_ai/hair dry.obj\"\n",
    "        obj2 = \"3d_eval_project/models/hunyuan3D/hair hunyuan.obj\"\n",
    "    \n",
    "    # Optional: force CPU for CLIP if you encounter memory issues\n",
    "    use_cpu = \"--cpu\" in sys.argv\n",
    "    \n",
    "    # Optional: adjust sample count\n",
    "    n_samples = 3000\n",
    "    for arg in sys.argv:\n",
    "        if arg.startswith(\"--samples=\"):\n",
    "            n_samples = int(arg.split(\"=\")[1])\n",
    "    \n",
    "    if use_cpu:\n",
    "        print(\"⚠️ Forcing CPU mode for CLIP\\n\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluate_models(obj1, obj2, n_samples=n_samples, use_cpu_clip=use_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c9a7d-782e-4f9f-8c14-4a7d92624307",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval3d-clean)",
   "language": "python",
   "name": "eval3d-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
