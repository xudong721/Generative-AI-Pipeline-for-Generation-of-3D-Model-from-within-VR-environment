{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5919a79-14e2-4ddb-992f-858d268dc40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa120h/miniforge3/envs/eval3d-clean/lib/python3.11/site-packages/clip/clip.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3D Model Evaluation - GTX 1080 Ti Optimized\n",
      "============================================================\n",
      "\n",
      "✓ CUDA available: NVIDIA GeForce GTX 1080 Ti\n",
      "✓ CUDA version: 11.8\n",
      "✓ Available memory: 11.71 GB\n",
      "\n",
      "Loading models...\n",
      "  Model 1: 3d_eval_project/models/tripo_ai/hair dry.obj\n",
      "  Model 2: 3d_eval_project/models/hunyuan3D/hair hunyuan.obj\n",
      "✓ Models loaded successfully\n",
      "  Model 1: 414278 vertices, 783386 faces\n",
      "  Model 2: 840587 vertices, 1499542 faces\n",
      "\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "1. Geometry: Chamfer Distance\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "✓ Chamfer Distance = 0.510184\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "2. Topology: Non-manifold Edge Ratio\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "✓ Model 1 non-manifold ratio = 0.0000\n",
      "✓ Model 2 non-manifold ratio = 0.0000\n",
      "\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "3. Perceptual: CLIP Similarity\n",
      "▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ ▶ \n",
      "Using GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "Loading CLIP model...\n",
      "Rendering views...\n",
      "✓ CLIP Similarity = 0.8785\n",
      "\n",
      "============================================================\n",
      "Summary\n",
      "============================================================\n",
      "  chamfer        : 0.510184\n",
      "  nm1            : 0.000000\n",
      "  nm2            : 0.000000\n",
      "  clip           : 0.878519\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "eval_no_viewer_gtx1080ti.py\n",
    "\n",
    "Optimized for GTX 1080 Ti (11GB VRAM, CUDA 6.1):\n",
    "- Reduced memory footprint for CLIP\n",
    "- FP32 precision (GTX 1080 Ti doesn't efficiently support FP16/BF16)\n",
    "- Batch processing with gradient disabled\n",
    "- CPU fallback options\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "import trimesh\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import clip\n",
    "import sys\n",
    "import gc\n",
    "\n",
    "sys.argv = [x for x in sys.argv if x != \"-f\"]\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Geometry: Chamfer Distance\n",
    "# -------------------------\n",
    "def chamfer_distance(mesh1, mesh2, n_samples=5000):\n",
    "    \"\"\"Compute Chamfer Distance with chunked processing for large point clouds\"\"\"\n",
    "    try:\n",
    "        pcd1 = mesh1.sample(n_samples)\n",
    "        pcd2 = mesh2.sample(n_samples)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Sampling failed ({e}), using vertices directly\")\n",
    "        pcd1 = mesh1.vertices[:n_samples]\n",
    "        pcd2 = mesh2.vertices[:n_samples]\n",
    "\n",
    "    # Use chunked processing for very large point clouds\n",
    "    chunk_size = 10000\n",
    "    if len(pcd1) > chunk_size or len(pcd2) > chunk_size:\n",
    "        print(f\"Using chunked Chamfer Distance computation...\")\n",
    "        return chamfer_distance_chunked(pcd1, pcd2, chunk_size)\n",
    "    \n",
    "    tree1 = cKDTree(pcd1)\n",
    "    tree2 = cKDTree(pcd2)\n",
    "\n",
    "    d12, _ = tree1.query(pcd2, k=1)\n",
    "    d21, _ = tree2.query(pcd1, k=1)\n",
    "\n",
    "    cd = float(np.mean(d12) + np.mean(d21))\n",
    "    return cd\n",
    "\n",
    "\n",
    "def chamfer_distance_chunked(pcd1, pcd2, chunk_size=10000):\n",
    "    \"\"\"Chunked version for memory efficiency\"\"\"\n",
    "    tree1 = cKDTree(pcd1)\n",
    "    tree2 = cKDTree(pcd2)\n",
    "    \n",
    "    # Query in chunks\n",
    "    d12_list = []\n",
    "    for i in range(0, len(pcd2), chunk_size):\n",
    "        chunk = pcd2[i:i+chunk_size]\n",
    "        d, _ = tree1.query(chunk, k=1)\n",
    "        d12_list.append(d)\n",
    "    d12 = np.concatenate(d12_list)\n",
    "    \n",
    "    d21_list = []\n",
    "    for i in range(0, len(pcd1), chunk_size):\n",
    "        chunk = pcd1[i:i+chunk_size]\n",
    "        d, _ = tree2.query(chunk, k=1)\n",
    "        d21_list.append(d)\n",
    "    d21 = np.concatenate(d21_list)\n",
    "    \n",
    "    cd = float(np.mean(d12) + np.mean(d21))\n",
    "    return cd\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Topology: non-manifold ratio\n",
    "# -------------------------\n",
    "def non_manifold_ratio(mesh):\n",
    "    \"\"\"Robust non-manifold edge detection\"\"\"\n",
    "    try:\n",
    "        edges = mesh.edges_unique\n",
    "        edge_faces = mesh.edges_face\n",
    "        \n",
    "        if edge_faces is None or len(edges) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        edge_faces = np.atleast_2d(edge_faces)\n",
    "        \n",
    "        # Count non-manifold edges (missing faces or multiple faces)\n",
    "        if edge_faces.ndim == 1:\n",
    "            edge_faces = edge_faces.reshape(-1, 1)\n",
    "        \n",
    "        non_manifold = np.sum(np.any(edge_faces == -1, axis=1))\n",
    "        ratio = float(non_manifold) / max(1, len(edges))\n",
    "        return ratio\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Non-manifold computation failed ({e}), returning 0.0\")\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CPU renderer for CLIP\n",
    "# -------------------------\n",
    "def render_pointcloud_views(mesh, image_size=224, point_radius=2, views=3):\n",
    "    \"\"\"\n",
    "    CPU-based orthographic projection rendering\n",
    "    Optimized for memory efficiency\n",
    "    \"\"\"\n",
    "    try:\n",
    "        verts = mesh.vertices.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not access vertices ({e})\")\n",
    "        return [Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))] * views\n",
    "    \n",
    "    if len(verts) == 0:\n",
    "        return [Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))] * views\n",
    "    \n",
    "    # Center and normalize\n",
    "    center = verts.mean(axis=0)\n",
    "    verts -= center\n",
    "    \n",
    "    scale = np.max(np.linalg.norm(verts, axis=1))\n",
    "    if scale <= 0:\n",
    "        scale = 1.0\n",
    "    verts = verts / scale\n",
    "    \n",
    "    imgs = []\n",
    "    projections = {\n",
    "        \"front\": verts[:, [0, 1]],\n",
    "        \"side\": verts[:, [2, 1]],\n",
    "        \"top\": verts[:, [0, 2]],\n",
    "    }\n",
    "    \n",
    "    selected = [\"front\", \"side\", \"top\"][:views]\n",
    "    for k in selected:\n",
    "        pts = projections[k]\n",
    "        margin = int(0.05 * image_size)\n",
    "        coords = ((pts + 1.0) * 0.5 * (image_size - 2*margin)) + margin\n",
    "        coords = np.round(coords).astype(int)\n",
    "        coords[:, 0] = np.clip(coords[:, 0], 0, image_size-1)\n",
    "        coords[:, 1] = np.clip(coords[:, 1], 0, image_size-1)\n",
    "        \n",
    "        img = Image.new(\"RGB\", (image_size, image_size), (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Draw points with anti-aliasing effect\n",
    "        for (x, y) in coords:\n",
    "            x0 = x - point_radius\n",
    "            y0 = image_size - 1 - y - point_radius\n",
    "            x1 = x + point_radius\n",
    "            y1 = image_size - 1 - y + point_radius\n",
    "            draw.ellipse([x0, y0, x1, y1], fill=(30, 30, 30))\n",
    "        \n",
    "        imgs.append(img)\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CLIP similarity - GTX 1080 Ti optimized\n",
    "# -------------------------\n",
    "def clip_similarity_from_meshes(mesh1, mesh2, image_size=224, device=None, use_cpu=False):\n",
    "    \"\"\"\n",
    "    CLIP similarity optimized for GTX 1080 Ti\n",
    "    - Reduced image size option\n",
    "    - CPU fallback\n",
    "    - Explicit memory management\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        if use_cpu or not torch.cuda.is_available():\n",
    "            device = \"cpu\"\n",
    "            print(\"Using CPU for CLIP (slower but safer for memory)\")\n",
    "        else:\n",
    "            device = \"cuda\"\n",
    "            print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    try:\n",
    "        # Clear GPU cache before loading\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Load CLIP model with FP32 (GTX 1080 Ti works best with FP32)\n",
    "        print(\"Loading CLIP model...\")\n",
    "        model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "        model.eval()  # Ensure eval mode\n",
    "        \n",
    "        # Render views\n",
    "        print(\"Rendering views...\")\n",
    "        imgs1 = render_pointcloud_views(mesh1, image_size=image_size)\n",
    "        imgs2 = render_pointcloud_views(mesh2, image_size=image_size)\n",
    "        \n",
    "        def combine(imgs):\n",
    "            widths, heights = zip(*(i.size for i in imgs))\n",
    "            total_w = sum(widths)\n",
    "            max_h = max(heights)\n",
    "            out = Image.new(\"RGB\", (total_w, max_h), (255, 255, 255))\n",
    "            x_offset = 0\n",
    "            for im in imgs:\n",
    "                out.paste(im, (x_offset, 0))\n",
    "                x_offset += im.size[0]\n",
    "            return out\n",
    "        \n",
    "        comb1 = combine(imgs1)\n",
    "        comb2 = combine(imgs2)\n",
    "        \n",
    "        # Preprocess images\n",
    "        image1 = preprocess(comb1).unsqueeze(0).to(device)\n",
    "        image2 = preprocess(comb2).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Compute similarity with no gradient\n",
    "        with torch.no_grad():\n",
    "            e1 = model.encode_image(image1).float()  # Explicit FP32\n",
    "            e2 = model.encode_image(image2).float()\n",
    "            \n",
    "            # Normalize\n",
    "            e1 = e1 / e1.norm(dim=-1, keepdim=True)\n",
    "            e2 = e2 / e2.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Compute similarity\n",
    "            sim = float((e1 @ e2.T).cpu().item())\n",
    "        \n",
    "        # Cleanup\n",
    "        del model, image1, image2, e1, e2\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return sim\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"\\n⚠️ GPU Out of Memory! Retrying with CPU...\")\n",
    "            if device == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            return clip_similarity_from_meshes(mesh1, mesh2, image_size, \"cpu\", True)\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Main evaluation\n",
    "# -------------------------\n",
    "def evaluate_models(obj1_path, obj2_path, n_samples=3000, use_cpu_clip=False):\n",
    "    \"\"\"\n",
    "    Evaluate two OBJ models\n",
    "    \n",
    "    Args:\n",
    "        obj1_path: Path to first OBJ file\n",
    "        obj2_path: Path to second OBJ file\n",
    "        n_samples: Number of points for Chamfer Distance (reduce if OOM)\n",
    "        use_cpu_clip: Force CPU for CLIP computation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"3D Model Evaluation - GTX 1080 Ti Optimized\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Check CUDA availability\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✓ CUDA available: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"✓ CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"✓ Available memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\\n\")\n",
    "    else:\n",
    "        print(\"⚠️ CUDA not available, using CPU\\n\")\n",
    "    \n",
    "    # Load meshes\n",
    "    print(f\"Loading models...\")\n",
    "    print(f\"  Model 1: {obj1_path}\")\n",
    "    print(f\"  Model 2: {obj2_path}\")\n",
    "    \n",
    "    try:\n",
    "        mesh1 = trimesh.load(obj1_path, force='mesh')\n",
    "        mesh2 = trimesh.load(obj2_path, force='mesh')\n",
    "        print(f\"✓ Models loaded successfully\")\n",
    "        print(f\"  Model 1: {len(mesh1.vertices)} vertices, {len(mesh1.faces)} faces\")\n",
    "        print(f\"  Model 2: {len(mesh2.vertices)} vertices, {len(mesh2.faces)} faces\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading models: {e}\")\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Chamfer Distance\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"1. Geometry: Chamfer Distance\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        cd = chamfer_distance(mesh1, mesh2, n_samples=n_samples)\n",
    "        print(f\"✓ Chamfer Distance = {cd:.6f}\")\n",
    "        results['chamfer'] = cd\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Chamfer Distance failed: {e}\")\n",
    "        results['chamfer'] = None\n",
    "    \n",
    "    # 2. Non-manifold ratio\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"2. Topology: Non-manifold Edge Ratio\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        nm1 = non_manifold_ratio(mesh1)\n",
    "        nm2 = non_manifold_ratio(mesh2)\n",
    "        print(f\"✓ Model 1 non-manifold ratio = {nm1:.4f}\")\n",
    "        print(f\"✓ Model 2 non-manifold ratio = {nm2:.4f}\")\n",
    "        results['nm1'] = nm1\n",
    "        results['nm2'] = nm2\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Non-manifold computation failed: {e}\")\n",
    "        results['nm1'] = None\n",
    "        results['nm2'] = None\n",
    "    \n",
    "    # 3. CLIP similarity\n",
    "    print(f\"\\n{'▶ '*30}\")\n",
    "    print(\"3. Perceptual: CLIP Similarity\")\n",
    "    print(f\"{'▶ '*30}\")\n",
    "    try:\n",
    "        clip_score = clip_similarity_from_meshes(\n",
    "            mesh1, mesh2, \n",
    "            image_size=224,\n",
    "            use_cpu=use_cpu_clip\n",
    "        )\n",
    "        print(f\"✓ CLIP Similarity = {clip_score:.4f}\")\n",
    "        results['clip'] = clip_score\n",
    "    except Exception as e:\n",
    "        print(f\"❌ CLIP computation failed: {e}\")\n",
    "        results['clip'] = None\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for k, v in results.items():\n",
    "        if v is not None:\n",
    "            print(f\"  {k:15s}: {v:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {k:15s}: FAILED\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Entry point\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    \n",
    "    # Parse arguments\n",
    "    if len(sys.argv) >= 3:\n",
    "        obj1 = sys.argv[1]\n",
    "        obj2 = sys.argv[2]\n",
    "    else:\n",
    "        obj1 = \"3d_eval_project/models/tripo_ai/hair dry.obj\"\n",
    "        obj2 = \"3d_eval_project/models/hunyuan3D/hair hunyuan.obj\"\n",
    "    \n",
    "    # Optional: force CPU for CLIP if you encounter memory issues\n",
    "    use_cpu = \"--cpu\" in sys.argv\n",
    "    \n",
    "    # Optional: adjust sample count\n",
    "    n_samples = 3000\n",
    "    for arg in sys.argv:\n",
    "        if arg.startswith(\"--samples=\"):\n",
    "            n_samples = int(arg.split(\"=\")[1])\n",
    "    \n",
    "    if use_cpu:\n",
    "        print(\"⚠️ Forcing CPU mode for CLIP\\n\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluate_models(obj1, obj2, n_samples=n_samples, use_cpu_clip=use_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e721d98-c36a-49fc-b3aa-23e4a1d71254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eval3d-clean)",
   "language": "python",
   "name": "eval3d-clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
